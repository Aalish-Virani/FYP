{"cells":[{"cell_type":"markdown","metadata":{"id":"WM1gpZe33pVA"},"source":["Importing required files"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ni5ibtjE_FQL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"17649a6b-f55f-470e-b165-1712825098ac","executionInfo":{"status":"ok","timestamp":1740479593973,"user_tz":-300,"elapsed":8213,"user":{"displayName":"MOhammad Haroon","userId":"09585794811129578628"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}],"source":["import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"]},{"cell_type":"markdown","source":["64ng G859+"],"metadata":{"id":"kof01Ze04gp1"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7cphvRB_GRD","outputId":"ca4d269f-0777-4795-aa1d-822ec97ac2da","executionInfo":{"status":"ok","timestamp":1740479608146,"user_tz":-300,"elapsed":12001,"user":{"displayName":"MOhammad Haroon","userId":"09585794811129578628"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n","CPU (s):\n","4.777949929000002\n","GPU (s):\n","0.1382895320000017\n","GPU speedup over CPU: 34x\n"]}],"source":["import tensorflow as tf\n","import timeit\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  print(\n","      '\\n\\nThis error most likely means that this notebook is not '\n","      'configured to use a GPU.  Change this in Notebook Settings via the '\n","      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n","  raise SystemError('GPU device not found')\n","\n","def cpu():\n","  with tf.device('/cpu:0'):\n","    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n","    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n","    return tf.math.reduce_sum(net_cpu)\n","\n","def gpu():\n","  with tf.device('/device:GPU:0'):\n","    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n","    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n","    return tf.math.reduce_sum(net_gpu)\n","\n","# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n","cpu()\n","gpu()\n","\n","# Run the op several times.\n","print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n","      '(batch x height x width x channel). Sum of ten runs.')\n","print('CPU (s):')\n","cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n","print(cpu_time)\n","print('GPU (s):')\n","gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n","print(gpu_time)\n","print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Di689Gat1cWz"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.layers import Embedding,Bidirectional, LSTM, Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import Adam\n","import pickle\n","import numpy as np\n","import os"]},{"cell_type":"markdown","metadata":{"id":"8v7b2TTc3nA9"},"source":["upload the corpas"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"AEb7_-xI1xj4","outputId":"fa8b1b9b-3b85-48a0-b743-3b27aeba01a1","executionInfo":{"status":"ok","timestamp":1735205763754,"user_tz":-300,"elapsed":41423,"user":{"displayName":"MOhammad Haroon","userId":"09585794811129578628"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-6d23b51d-a69d-40dc-98f6-0ccdea4d937a\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-6d23b51d-a69d-40dc-98f6-0ccdea4d937a\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving 3000lines.txt to 3000lines.txt\n"]}],"source":["from google.colab import files\n","uploaded = files.upload()"]},{"cell_type":"markdown","metadata":{"id":"XhW-fEceNqAC"},"source":["# New section"]},{"cell_type":"markdown","metadata":{"id":"p3ucYDm434P5"},"source":["Open and pre-process the data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7e07CYb39zL","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"2dcf8360-0b7f-4aff-910a-3bdf585a0f65","executionInfo":{"status":"error","timestamp":1735205943671,"user_tz":-300,"elapsed":1213,"user":{"displayName":"MOhammad Haroon","userId":"09585794811129578628"}}},"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"unindent does not match any outer indentation level (<tokenize>, line 9)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    for i in lines:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"]}],"source":["file = open(\"3000lines.txt\", \"r\", encoding = \"utf8\")\n","\n","# store file in list\n","lines = []\n","for i in file:\n","    lines.append(i)\n","\n","# Convert list to string\n"," for i in lines:\n","  data = ' '. join(lines)\n","\n","#replace unnecessary stuff with space\n","data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n","\n","#remove unnecessary spaces\n","data = data.split()\n","data = ' '.join(data)\n","data[:500]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRljgVB_4Vm6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"75048bb9-55e5-4e9a-a9e7-69008243a810"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["230299"]},"metadata":{},"execution_count":38}],"source":["len(data)"]},{"cell_type":"markdown","metadata":{"id":"ESHWTabm4geY"},"source":["Apply tokenization and some other changes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oUUibJJu4xvw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dd2d09a1-6f3a-453f-caea-56d975b681a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["The Length of sequences are:  52767\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[  12,   40,  227,    3],\n","       [  40,  227,    3,   13],\n","       [ 227,    3,   13,   23],\n","       [   3,   13,   23,   45],\n","       [  13,   23,   45,  657],\n","       [  23,   45,  657,   12],\n","       [  45,  657,   12,  658],\n","       [ 657,   12,  658,  859],\n","       [  12,  658,  859,    4],\n","       [ 658,  859,    4, 1749]])"]},"metadata":{},"execution_count":42}],"source":["sequences = []\n","\n","for i in range(3, len(sequence_data)):\n","    words = sequence_data[i-3:i+1]\n","    sequences.append(words)\n","\n","print(\"The Length of sequences are: \", len(sequences))\n","sequences = np.array(sequences)\n","sequences[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKYKV23b41iF"},"outputs":[],"source":["X = []\n","y = []\n","\n","for i in sequences:\n","    X.append(i[0:3])\n","    y.append(i[3])\n","\n","X = np.array(X)\n","y = np.array(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-MwMLXFq49IY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3fa181db-effa-4f12-8a56-1af6d32596b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data:  [[ 12  40 227]\n"," [ 40 227   3]\n"," [227   3  13]\n"," [  3  13  23]\n"," [ 13  23  45]\n"," [ 23  45 657]\n"," [ 45 657  12]\n"," [657  12 658]\n"," [ 12 658 859]\n"," [658 859   4]]\n","Response:  [   3   13   23   45  657   12  658  859    4 1749]\n"]}],"source":["print(\"Data: \", X[:10])\n","print(\"Response: \", y[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3nWkdm44_Fy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"741a7be1-86e8-477b-8704-c24d77706ee4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"]},"metadata":{},"execution_count":45}],"source":["y = to_categorical(y, num_classes=vocab_size)\n","y[:5]"]},{"cell_type":"markdown","metadata":{"id":"Un_SZ40e5Eqw"},"source":[" Creating the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdA-vGXS5FbP"},"outputs":[],"source":["model = Sequential()\n","model.add(Embedding(vocab_size, 10, input_length=3))\n","model.add (Bidirectional(LSTM(256)))\n","model.add(Dense(1000, activation=\"relu\"))\n","model.add(Dense(vocab_size, activation=\"softmax\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZIgCW4tZ5MWZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d7500e9a-50c5-4d88-e7c9-42dd152c58dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_3 (Embedding)     (None, 3, 10)             89890     \n","                                                                 \n"," bidirectional (Bidirection  (None, 512)               546816    \n"," al)                                                             \n","                                                                 \n"," dense_2 (Dense)             (None, 1000)              513000    \n","                                                                 \n"," dense_3 (Dense)             (None, 8989)              8997989   \n","                                                                 \n","=================================================================\n","Total params: 10147695 (38.71 MB)\n","Trainable params: 10147695 (38.71 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3ukcoTw5RHu"},"outputs":[],"source":["# from tensorflow import keras\n","# from keras.utils.vis_utils import plot_model\n","\n","# keras.utils.plot_model(model, to_file='plot.png', show_layer_names=True)"]},{"cell_type":"markdown","metadata":{"id":"S533CYvZ5a6U"},"source":["Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rks9_8nk5bnO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"afb1f34b-7a0a-4e0d-d4cd-be899d490e96"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/60\n","317/660 [=============>................] - ETA: 8s - loss: 7.4203 - accuracy: 0.0312"]}],"source":["from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='Accuracy', verbose=1, save_best_only=False )\n","model.compile(loss=\"categorical_crossentropy\", metrics=['accuracy'],optimizer=Adam(learning_rate=0.001))\n","history = model.fit(X, y, validation_split= 0.2, epochs=60, batch_size=64, callbacks=[checkpoint])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"klXIzl1RVZwz"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(history.history['accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epochs')\n","plt.legend(['accuracy'],loc ='upper left')\n","plt.show"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zz-Rtk5yVbI9"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(history.history['loss'])\n","plt.title('Model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epochs')\n","plt.legend(['loss'],loc ='upper left')\n","plt.show"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuOgHFqJVjCX"},"outputs":[],"source":["y_hat = model.predict(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Li-jktiLVusm"},"outputs":[],"source":["y_pred = np.argmax(y_hat, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3k1ppqJMYOLC"},"outputs":[],"source":["len(y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvZTTFIF5opc"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","import numpy as np\n","import pickle\n","\n","# Load the model and tokenizer\n","model = load_model('next_words.h5')\n","tokenizer = pickle.load(open('token.pkl', 'rb'))\n","\n","def Predict_Next_Words(model, tokenizer, text):\n","\n","  sequence = tokenizer.texts_to_sequences([text])\n","  sequence = np.array(sequence)\n","  preds = np.argmax(model.predict(sequence))\n","  predicted_word = \"\"\n","\n","  for key, value in tokenizer.word_index.items():\n","      if value == preds:\n","          predicted_word = key\n","          break\n","\n","  print(predicted_word)\n","  return predicted_word"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6UpTi5omo6J"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","\n","def plot_graphs(history, string):\n","    plt.plot(history.history[string])\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(string)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BDc9V_ob5yXu"},"outputs":[],"source":["while(True):\n","  text = input(\"Enter your line: \")\n","\n","  if text == \"0\":\n","      print(\"Execution completed.....\")\n","      break\n","\n","  else:\n","      try:\n","          text = text.split(\" \")\n","          text = text[:3]\n","          print(text)\n","\n","          Predict_Next_Words(model, tokenizer, text)\n","\n","      except Exception as e:\n","        print(\"Error occurred: \",e)\n","        continue"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}